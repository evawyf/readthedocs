# Customize Losses


The final model-specific component is the Loss Function. The loss function is a function that defines the learning curve of the neural network and is responsible for adjusting the network weights. A key attribute of the loss function is that it can be removed from the graph when switching from training modes to testing modes. This means that after the weights have been finalized by minimizing the loss function, the loss function itself is no longer required. Thus, the loss function is tested during model training. 
The loss function must be differentiable because it defines the gradient flow through the neural network by backpropagation. Therefore, the loss function must only use differentiable operations from the TensorFlow library, such as those found in the tf.math, tf.nn, tf.keras.backend, or the tf.keras.losses APIs. 

The loss function is typically more intricate in terms of being automatically derived than other components like the layers or the blocks themselves. This means that when computing the derivative, TensorFlow must keep track of more variables and as such it must use more memory and take up more time. Generally speaking, not all computations in the loss function need to be derived, thus to keep the loss function and the overall footprint of the loss function in check, it is best to tell TensorFlow what not to pay attention to when computing your derivatives. This can be accomplished by using the tf.stop_gradient method. This operation will flag any tensor such that the gradient tape will view it as a constant, and ignore any computation that lead to the creation of the flagged tensor. Since it is a mathematical function, the loss function derivative can be manually computed and verified. 
